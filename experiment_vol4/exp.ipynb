{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06eea373-df47-47a5-ac02-a017d973c5c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-29T01:52:07.145997Z",
     "iopub.status.busy": "2023-08-29T01:52:07.145843Z",
     "iopub.status.idle": "2023-08-29T01:52:07.150168Z",
     "shell.execute_reply": "2023-08-29T01:52:07.148979Z",
     "shell.execute_reply.started": "2023-08-29T01:52:07.145982Z"
    },
    "tags": []
   },
   "source": [
    "实验Noisy Channel Model、Contextual Calibration、Domain-context Calibration的效果\n",
    "\n",
    "实验数据集\n",
    "\n",
    "* TNEWS\n",
    "* NLPCC2014-task2\n",
    "\n",
    "LLM\n",
    "\n",
    "* Qwen-7B-Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c66db00-7b71-4245-9e0d-d70826c70e73",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from paddlenlp import Taskflow\n",
    "from modelscope.utils.constant import Tasks\n",
    "from modelscope import Model\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "from modelscope import GenerationConfig\n",
    "\n",
    "\n",
    "MODEL_NAME = 'qwen-7b-chat' ## [qwen-7b-chat, siamese_uninlu, paddle_nlp]\n",
    "DATASET = 'tnews' # [tnews, nlpcc2014_task2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483f89fe-8e22-44ad-be58-cd7e488a0188",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 读取数据集\n",
    "import json\n",
    "\n",
    "train, val, test = [], [], []\n",
    "\n",
    "if DATASET == 'tnews':\n",
    "    with open('dataset/tnews/train.json', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = json.loads(line.strip())\n",
    "            label = line['label']\n",
    "            text = line['sentence']\n",
    "            train.append([text, label])\n",
    "            \n",
    "    with open('dataset/tnews/dev.json', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = json.loads(line.strip())\n",
    "            label = line['label']\n",
    "            text = line['sentence']\n",
    "            val.append([text, label])\n",
    "            \n",
    "    # 原数据集已随机打散过\n",
    "    test = val[:int(len(val))//2]\n",
    "    val = val[int(len(val)//2):]\n",
    "    \n",
    "    id2en = {}\n",
    "    en2id = {}\n",
    "    \n",
    "    with open('dataset/tnews/labels.json', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = json.loads(line.strip())\n",
    "            id2en[line['label']] = line['label_desc']\n",
    "            en2id[line['label_desc']] = line['label']\n",
    "        \n",
    "    \n",
    "    en2zh = {\n",
    "        'news_story': '故事',\n",
    "        'news_culture': '文化新闻',\n",
    "        'news_entertainment': '娱乐新闻',\n",
    "        'news_sports': '体育新闻',\n",
    "        'news_finance': '经济新闻',\n",
    "        'news_house': '房地产新闻',\n",
    "        'news_car': '汽车新闻',\n",
    "        'news_edu': '教育新闻',\n",
    "        'news_tech': '科技新闻',\n",
    "        'news_military': '军事新闻',\n",
    "        'news_travel': '旅游新闻',\n",
    "        'news_world': '国际新闻',\n",
    "        'news_stock': '股市新闻',\n",
    "        'news_agriculture': '农业新闻',\n",
    "        'news_game': '游戏新闻'\n",
    "    }\n",
    "    \n",
    "    zh2en = {}\n",
    "    for en_name in en2zh:\n",
    "        zh2en[en2zh[en_name]] = en_name\n",
    "        \n",
    "    zh_label_name_list = sorted(list(zh2en.keys()))\n",
    "    en_label_name_list = sorted(list(en2zh.keys()))\n",
    "    \n",
    "    print(f'{DATASET}数据集加载完成!')\n",
    "    print(f'训练集: {len(train)}, 验证集: {len(val)}, 测试集: {len(test)}')\n",
    "\n",
    "elif DATASET == 'nlpcc2014_task2':\n",
    "    \n",
    "    id2en = {\n",
    "        0: 'negative',\n",
    "        1: 'positive'\n",
    "    }\n",
    "    \n",
    "    en2id = {}\n",
    "    for label_id in id2en:\n",
    "        en2id[id2en[label_id]] = label_id\n",
    "    \n",
    "    with open('dataset/NLPCC2014_task2/train.json', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = json.loads(line.strip())\n",
    "            text, label = line['review'], en2id[line['label']]\n",
    "            train.append([text, label])\n",
    "    with open('dataset/NLPCC2014_task2/val.json', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = json.loads(line.strip())\n",
    "            text, label = line['review'], en2id[line['label']]\n",
    "            val.append([text, label])\n",
    "    with open('dataset/NLPCC2014_task2/test.json', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = json.loads(line.strip())\n",
    "            text, label = line['review'], en2id[line['label']]\n",
    "            test.append([text, label])\n",
    "    \n",
    "    en2zh = {\n",
    "        'positive': '正面情绪',\n",
    "        'negative': '负面情绪'\n",
    "    }\n",
    "    \n",
    "    zh2en = {}\n",
    "    for en_name in en2zh:\n",
    "        zh2en[en2zh[en_name]] = en_name\n",
    "        \n",
    "    zh_label_name_list = sorted(list(zh2en.keys()))\n",
    "    en_label_name_list = sorted(list(en2zh.keys()))\n",
    "    \n",
    "    print(f'{DATASET}数据集加载完成!')\n",
    "    print(f'训练集: {len(train)}, 验证集: {len(val)}, 测试集: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb08850-ecfe-44ad-bf45-fe9f3566ee33",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if MODEL_NAME == 'qwen-7b-chat':\n",
    "    # 设置生成参数直接设置model.generation_config\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"qwen/Qwen-7B-Chat\", revision = 'v1.0.5',trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"qwen/Qwen-7B-Chat\", revision = 'v1.0.5',device_map=\"auto\", trust_remote_code=True,fp16 = True).eval()\n",
    "    model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\",revision = 'v1.0.5', trust_remote_code=True) \n",
    "    \n",
    "    print(f'{MODEL_NAME}模型加载成功')\n",
    "    def get_llm_result(prompt, mode):\n",
    "        if mode == 'greedy_decode':\n",
    "            model.generation_config.do_sample = False\n",
    "        elif mode == 'sample':\n",
    "            model.generation_config.do_sample = True\n",
    "        response, history = model.chat(tokenizer, prompt, history=None)\n",
    "        return response\n",
    "    \n",
    "elif MODEL_NAME == 'siamese_uninlu':\n",
    "    semantic_cls = pipeline(Tasks.siamese_uie, 'damo/nlp_structbert_siamese-uninlu_chinese-base', model_revision='v1.0')\n",
    "    print(f'{MODEL_NAME}模型加载完成！')\n",
    "    \n",
    "    if DATASET in ['tnews', 'nlpcc2014_task2']:\n",
    "        schema = {'分类': None}\n",
    "\n",
    "    def get_siamase_result(text, label_names):\n",
    "        res = semantic_cls(input=','.join(label_names)+'|'+text, schema = schema)['output']\n",
    "        if len(res) > 0:\n",
    "            return res[0][0]['span']\n",
    "        else:\n",
    "            return ''\n",
    "        \n",
    "elif MODEL_NAME == 'paddle_nlp':\n",
    "    if DATASET in ['tnews', 'nlpcc2014_task2']:\n",
    "        schema = zh_label_name_list\n",
    "        model = Taskflow(\"zero_shot_text_classification\", schema=schema)\n",
    "        print(f'{MODEL_NAME}模型加载完成！')\n",
    "    \n",
    "    def get_paddle_result(text):\n",
    "        res = model(text)\n",
    "        if len(res) > 0:\n",
    "            if DATASET in ['tnews', 'nlpcc2014_task2']:\n",
    "                if len(res[0]['predictions']) > 0:\n",
    "                    return res[0]['predictions'][0]['label']\n",
    "        \n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eb31ee4-d334-44b9-b4bc-8cc70ad6b079",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2023-09-01T03:15:42.928332Z",
     "iopub.status.busy": "2023-09-01T03:15:42.927960Z",
     "iopub.status.idle": "2023-09-01T03:15:42.943248Z",
     "shell.execute_reply": "2023-09-01T03:15:42.942493Z",
     "shell.execute_reply.started": "2023-09-01T03:15:42.928310Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 为实现拿到output prob所需要的utils\n",
    "# 根据根据input text构建模型需要的input格式。本质是自己实现预处理+forward，通过查看modelscope中，每个模型text_generation逻辑来实现\n",
    "\n",
    "# Qwen\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import PreTrainedTokenizer\n",
    "from typing import Iterable, List, Tuple, Union\n",
    "\n",
    "def make_context_qwen(\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    query: str,\n",
    "    history: List[Tuple[str, str]] = None,\n",
    "    system: str = '',\n",
    "    max_window_size: int = 6144,\n",
    "    chat_format: str = 'chatml',\n",
    "):\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    if chat_format == 'chatml':\n",
    "        im_start, im_end = '<|im_start|>', '<|im_end|>'\n",
    "        im_start_tokens = [tokenizer.im_start_id]\n",
    "        im_end_tokens = [tokenizer.im_end_id]\n",
    "        nl_tokens = tokenizer.encode('\\n')\n",
    "\n",
    "        def _tokenize_str(role, content):\n",
    "            return f'{role}\\n{content}', tokenizer.encode(\n",
    "                role) + nl_tokens + tokenizer.encode(content)\n",
    "\n",
    "        system_text, system_tokens_part = _tokenize_str('system', system)\n",
    "        system_tokens = im_start_tokens + system_tokens_part + im_end_tokens\n",
    "\n",
    "        raw_text = ''\n",
    "        context_tokens = []\n",
    "\n",
    "        for turn_query, turn_response in reversed(history):\n",
    "            query_text, query_tokens_part = _tokenize_str('user', turn_query)\n",
    "            query_tokens = im_start_tokens + query_tokens_part + im_end_tokens\n",
    "            response_text, response_tokens_part = _tokenize_str(\n",
    "                'assistant', turn_response)\n",
    "            response_tokens = im_start_tokens + response_tokens_part + im_end_tokens\n",
    "\n",
    "            next_context_tokens = nl_tokens + query_tokens + nl_tokens + response_tokens\n",
    "            prev_chat = (\n",
    "                f'\\n{im_start}{query_text}{im_end}\\n{im_start}{response_text}{im_end}'\n",
    "            )\n",
    "\n",
    "            current_context_size = (\n",
    "                len(system_tokens) + len(next_context_tokens)\n",
    "                + len(context_tokens))\n",
    "            if current_context_size < max_window_size:\n",
    "                context_tokens = next_context_tokens + context_tokens\n",
    "                raw_text = prev_chat + raw_text\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        context_tokens = system_tokens + context_tokens\n",
    "        raw_text = f'{im_start}{system_text}{im_end}' + raw_text\n",
    "        context_tokens += (\n",
    "            nl_tokens + im_start_tokens + _tokenize_str('user', query)[1]\n",
    "            + im_end_tokens + nl_tokens + im_start_tokens\n",
    "            + tokenizer.encode('assistant') + nl_tokens)\n",
    "        raw_text += f'\\n{im_start}user\\n{query}{im_end}\\n{im_start}assistant\\n'\n",
    "\n",
    "    elif chat_format == 'raw':\n",
    "        raw_text = query\n",
    "        context_tokens = tokenizer.encode(raw_text)\n",
    "    else:\n",
    "        raise NotImplementedError(f'Unknown chat format {chat_format!r}')\n",
    "\n",
    "    return raw_text, context_tokens\n",
    "\n",
    "def concat_answer_qwen(raw_text, context_tokens, answer):\n",
    "    raw_text += answer\n",
    "    context_tokens += tokenizer.encode(answer)\n",
    "    \n",
    "    return raw_text, context_tokens\n",
    "\n",
    "def get_stop_words_ids_qwen(chat_format, tokenizer):\n",
    "    if chat_format == 'raw':\n",
    "        stop_words_ids = [tokenizer.encode('Human:'), [tokenizer.eod_id]]\n",
    "    elif chat_format == 'chatml':\n",
    "        stop_words_ids = [[tokenizer.im_end_id], [tokenizer.im_start_id]]\n",
    "    else:\n",
    "        raise NotImplementedError(f'Unknown chat format {chat_format!r}')\n",
    "    return stop_words_ids\n",
    "\n",
    "def get_answer_prob_qwen(prompt, answer):\n",
    "    # 得到问题部分的prob，通过perplexity计算得到\n",
    "    # 这部分代码参考https://github.com/tonyzhaozh/few-shot-learning\n",
    "\n",
    "    assert answer != None and len(answer) > 0\n",
    "\n",
    "    prompt = [prompt]\n",
    "    \n",
    "    raw_text, context_tokens = make_context_qwen(\n",
    "                tokenizer,\n",
    "                query=prompt[0],\n",
    "                history=[],\n",
    "                system='You are a helpful assistant.',\n",
    "                max_window_size=6144,\n",
    "                chat_format=model.generation_config.chat_format)\n",
    "    stop_words_ids = get_stop_words_ids_qwen(model.generation_config.chat_format, tokenizer)\n",
    "    \n",
    "    answer_start = len(context_tokens)\n",
    "    raw_text, context_tokens = concat_answer_qwen(raw_text, context_tokens, answer)\n",
    "    answer_end = len(context_tokens)\n",
    "    answer_len = answer_end - answer_start\n",
    "    \n",
    "    input_ids = torch.tensor([context_tokens]).to('cuda:0')\n",
    "\n",
    "    # 不等号右侧本来应该为pad id，但qwen没有设置pad token\n",
    "    attention_mask = (input_ids != -1).float()\n",
    "    input_for_gen = model.prepare_inputs_for_generation(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    logits = model.forward(input_ids=input_for_gen['input_ids'], attention_mask=input_for_gen['attention_mask'], position_ids=input_for_gen['position_ids'], return_dict=True).logits.detach()\n",
    "    probs = torch.softmax(logits, dim=2)\n",
    "        \n",
    "    logprobs = torch.log(probs)\n",
    "\n",
    "    return_json = {}\n",
    "    choices = []\n",
    "    for batch_id in range(len(prompt)):\n",
    "        curr_json = {}\n",
    "        # text is just the optional context and next l tokens\n",
    "        curr_json['text'] = tokenizer.decode(input_ids[batch_id], skip_special_tokens=True)\n",
    "\n",
    "        curr_json['logprobs'] = {}\n",
    "        curr_json['logprobs']['token_logprobs'] = []\n",
    "        curr_json['logprobs']['tokens'] = []\n",
    "        for index in range(len(probs[batch_id])):\n",
    "            curr_json['logprobs']['tokens'].append(tokenizer.decode([input_ids[batch_id][index]]))\n",
    "        curr_json['logprobs']['token_logprobs'].append('null')\n",
    "        for index, log_probs_token_position_j in enumerate(logprobs[batch_id][:-1]):\n",
    "            # probs are left shifted for LMs \n",
    "            curr_json['logprobs']['token_logprobs'].append(log_probs_token_position_j[input_ids[batch_id][index+1]])\n",
    "                \n",
    "        choices.append(curr_json)\n",
    "\n",
    "    return_json['choices'] = choices\n",
    "    \n",
    "    # 得到answer部分的perplexity\n",
    "    answer_logprob = 0.0\n",
    "    for answer_index in range(answer_start, answer_end):\n",
    "        answer_logprob += choices[0]['logprobs']['token_logprobs'][answer_index].cpu()\n",
    "    answer_logprob /= answer_len # length-normalize\n",
    "    answer_prob = np.exp(answer_logprob)\n",
    "                                  \n",
    "    return return_json, answer_prob.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "389defbe-44ab-4f9e-a8e2-04b2d89e1e54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T03:15:42.944443Z",
     "iopub.status.busy": "2023-09-01T03:15:42.944165Z",
     "iopub.status.idle": "2023-09-01T03:15:42.950302Z",
     "shell.execute_reply": "2023-09-01T03:15:42.949465Z",
     "shell.execute_reply.started": "2023-09-01T03:15:42.944423Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt\n",
    "def get_prompt(input_text, label_name_list, few_shot_data = []):\n",
    "    if len(few_shot_data) > 0:\n",
    "        example = ''\n",
    "        for d in few_shot_data:\n",
    "            text, label = d[0], d[1]\n",
    "            example += f\"文本:'{text}'\\n\"\n",
    "            example += f'这段文本所属标签：{label}\\n'\n",
    "        \n",
    "        prompt = f'''\n",
    "        给定一段文本，输出一个分类标签。\n",
    "        分类标签集合：[{','.join(label_name_list)}]\n",
    "        请直接输出结果，不要附带其他内容。\n",
    "        \n",
    "        {example}\n",
    "        文本：'{input_text}'\n",
    "        这段文本所属标签：\n",
    "        '''\n",
    "    else:\n",
    "        prompt = f'''\n",
    "        给定一段文本，输出一个分类标签。\n",
    "        分类标签集合：[{','.join(label_name_list)}]\n",
    "        请直接输出结果，不要附带其他内容。\n",
    "        \n",
    "        文本：'{input_text}'\n",
    "        这段文本所属标签：\n",
    "        '''\n",
    "        \n",
    "    return prompt\n",
    "\n",
    "def get_reverse_prompt(label_name, few_shot_data = []):\n",
    "    if len(few_shot_data) > 0:\n",
    "        example = ''\n",
    "        for d in few_shot_data:\n",
    "            text, label = d[0], d[1]\n",
    "            example += f'标签：{label}\\n'\n",
    "            example += f\"生成文本：{text}\\n\"\n",
    "\n",
    "        prompt = f'''\n",
    "        给定一个分类标签，生成一段文本。\n",
    "        请直接输出结果，不要附带其他内容。\n",
    "\n",
    "        {example}\n",
    "        标签：{label_name}\n",
    "        生成文本：\n",
    "        '''\n",
    "    else:\n",
    "        prompt = f'''\n",
    "        给定一个分类标签，生成一段文本。\n",
    "        请直接输出内容，不要附带其他内容。\n",
    "\n",
    "        标签：{label_name}\n",
    "        生成文本：\n",
    "        '''\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfd1b415-306a-406b-a92d-080d674ac831",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-01T03:15:42.951372Z",
     "iopub.status.busy": "2023-09-01T03:15:42.951111Z",
     "iopub.status.idle": "2023-09-01T03:15:42.957138Z",
     "shell.execute_reply": "2023-09-01T03:15:42.956442Z",
     "shell.execute_reply.started": "2023-09-01T03:15:42.951354Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# decode生成的utils\n",
    "import re \n",
    "\n",
    "def post_process_llm_cls(llm_out, label_name_list):\n",
    "    # 在分类任务中，对llm out进行后处理，提升coverage\n",
    "    post_process_pat = re.compile(r'(' + r'|'.join(label_name_list) + r')')\n",
    "    # 针对多生成问题\n",
    "    find_names = post_process_pat.findall(llm_out)\n",
    "    if len(find_names) > 0:\n",
    "        # 若有多个结果，默认取第一个\n",
    "        return find_names[0]\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def post_process_siamase_cls(out, label_name_list):\n",
    "    # 在分类任务中，对siamase out进行后处理，提升coverage\n",
    "    for label_name in label_name_list:\n",
    "        if out in label_name:\n",
    "            # 针对片段式抽取问题\n",
    "            return label_name\n",
    "    \n",
    "    return ''\n",
    "    \n",
    "def siamase_annotator_cls(text, label_name_list, print_log = False):\n",
    "    out = get_siamase_result(text, label_name_list)\n",
    "    if print_log:\n",
    "        print('out:', out)\n",
    "        \n",
    "    if len(out) > 0 and out not in label_name_list:\n",
    "        out = post_process_siamase_cls(out, label_name_list)\n",
    "        if print_log:\n",
    "            print('post-process out:', out)\n",
    "    \n",
    "    if len(out) > 0 and out in label_name_list:\n",
    "        return out\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def paddle_annotator_cls(text, label_name_list, print_log = False):\n",
    "    out = get_paddle_result(text)\n",
    "    if print_log:\n",
    "        print('out:', out)\n",
    "        \n",
    "    if len(out) > 0 and out in label_name_list:\n",
    "        return out\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a9eadfa-551e-43bb-bbd0-fab3ffe097a2",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2023-09-01T03:15:42.958448Z",
     "iopub.status.busy": "2023-09-01T03:15:42.958125Z",
     "iopub.status.idle": "2023-09-01T03:15:42.972559Z",
     "shell.execute_reply": "2023-09-01T03:15:42.971811Z",
     "shell.execute_reply.started": "2023-09-01T03:15:42.958431Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 模型标注方法\n",
    "def llm_annotator_generative_cls(input_text, mode, label_name_list, few_shot_data = [], self_consistency_num = 1, print_log=False):\n",
    "    \n",
    "    prompt = get_prompt(input_text, label_name_list, few_shot_data)\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    for i in range(self_consistency_num):\n",
    "        llm_out = get_llm_result(prompt, mode)\n",
    "        if print_log:\n",
    "            print('prompt:', prompt)\n",
    "            print('llm_out:', llm_out)\n",
    "            \n",
    "            \n",
    "        if llm_out not in label_name_list:\n",
    "            # 增加后处理\n",
    "            llm_out = post_process_llm_cls(llm_out, label_name_list)\n",
    "            if print_log:\n",
    "                print('post-process llm_out:', llm_out)\n",
    "        \n",
    "        if llm_out in label_name_list:\n",
    "            if llm_out not in result:\n",
    "                result[llm_out] = 1\n",
    "            else:\n",
    "                result[llm_out] += 1\n",
    "                  \n",
    "    if len(result) > 0:\n",
    "        if print_log:\n",
    "            print('self-consistency result:', result)\n",
    "            print('='*30)\n",
    "            print()\n",
    "        result = sorted(result.items(), key = lambda x: x[1], reverse=True)\n",
    "        return result[0][0]\n",
    "    else:\n",
    "        if print_log:\n",
    "            print('self-consistency result:', result)\n",
    "            print('='*30)\n",
    "            print()\n",
    "        return ''\n",
    "    \n",
    "def llm_annotator_channel(input_text, label_name_list, few_shot_data = [], print_log=False):\n",
    "    # noisy channel方法用来标注\n",
    "    # 用p(x|y)来选择y\n",
    "    \n",
    "    if print_log:\n",
    "        print(f'input_text:{input_text}')\n",
    "        print()\n",
    "    \n",
    "    label_probs = []\n",
    "    for label_name in label_name_list:\n",
    "        prompt = get_reverse_prompt(label_name, few_shot_data)\n",
    "        \n",
    "        _, x_prob = get_answer_prob_qwen(prompt, input_text)\n",
    "        label_probs.append(x_prob)\n",
    "        \n",
    "        if print_log:\n",
    "            print(f'label_name:{label_name}, label_prob:{x_prob}')\n",
    "        \n",
    "    if print_log:\n",
    "        print(f'pred_label_name:{label_name_list[np.argmax(label_probs)]}, pred_label_prob:{label_probs[np.argmax(label_probs)]}')\n",
    "        print('='*30)\n",
    "        print()\n",
    "    \n",
    "    return label_name_list[np.argmax(label_probs)]\n",
    "\n",
    "\n",
    "def llm_annotator_probmodel(input_text, label_name_list, few_shot_data = [], print_log=False):\n",
    "    # 从概率的视角，进行标注\n",
    "    \n",
    "    if print_log:\n",
    "        print(f'input_text:{input_text}')\n",
    "        print()\n",
    "        \n",
    "    label_probs = []\n",
    "    \n",
    "    for label_name in label_name_list:\n",
    "        prompt = get_prompt(input_text, label_name_list, few_shot_data)\n",
    "    \n",
    "        _, label_prob = get_answer_prob_qwen(prompt, label_name)\n",
    "        label_probs.append(label_prob)\n",
    "        \n",
    "        if print_log:\n",
    "            print(f'label_name:{label_name}, label_prob:{label_prob}')\n",
    "        \n",
    "    if print_log:\n",
    "        print(f'pred_label_name:{label_name_list[np.argmax(label_probs)]}, pred_label_prob:{label_probs[np.argmax(label_probs)]}')\n",
    "        print('='*30)\n",
    "        print()\n",
    "    \n",
    "    return label_probs, label_name_list[np.argmax(label_probs)]\n",
    "\n",
    "def get_p_calibrate_CC(label_name_list, few_shot_data = [], content_free_tokens = [''], print_log = False):\n",
    "    # CC方法中，计算p calibrate的逻辑，对于固定的prompt（非input text部分），只需要计算一次\n",
    "    all_p_calibrate = []\n",
    "    for content_free_token in content_free_tokens:\n",
    "        probs, _ = llm_annotator_probmodel(content_free_token, label_name_list, few_shot_data, print_log)\n",
    "        all_p_calibrate.append(probs)\n",
    "        \n",
    "    p_calibrate = np.mean(np.array(all_p_calibrate), axis=0)\n",
    "    p_calibrate = p_calibrate / np.sum(p_calibrate) # normalize\n",
    "    \n",
    "    return p_calibrate\n",
    "\n",
    "def get_p_calibrate_DC(label_name_list, dc_len, dc_times, domain_words, few_shot_data = [], print_log = False):\n",
    "    # DC方法中，计算p calibrate的逻辑：从domain words中随机选dc_len个词，重复dc_times次\n",
    "    all_p_calibrate = []\n",
    "    for i in range(dc_times):\n",
    "        dc_words = np.random.choice(domain_words, dc_len, replace = False)\n",
    "        if print_log:\n",
    "            print(''.join(dc_words))\n",
    "        probs, _ = llm_annotator_probmodel(''.join(dc_words), label_name_list, few_shot_data, print_log = False)\n",
    "        all_p_calibrate.append(probs)\n",
    "        \n",
    "    p_calibrate = np.mean(np.array(all_p_calibrate), axis=0)\n",
    "    p_calibrate = p_calibrate / np.sum(p_calibrate) # normalize\n",
    "    \n",
    "    return p_calibrate\n",
    "\n",
    "def llm_annotator_calibrate(input_text, p_calibrate, label_name_list, few_shot_data = [], print_log=False):\n",
    "    # calibrate方法\n",
    "    if print_log:\n",
    "        print(f'input_text:{input_text}')\n",
    "        print()\n",
    "        \n",
    "    probs, _ = llm_annotator_probmodel(input_text, label_name_list, few_shot_data, print_log=False)\n",
    "    num_classes = len(label_name_list)\n",
    "    \n",
    "    # diagonal_W，即除法\n",
    "    W = np.linalg.inv(np.identity(num_classes) * p_calibrate)\n",
    "    b = np.zeros([num_classes, 1])\n",
    "    \n",
    "    probs = probs / np.sum(probs) # normalize\n",
    "    calibrate_label_probs = np.matmul(W, np.expand_dims(probs, axis=-1)) + b\n",
    "    \n",
    "    if print_log:\n",
    "        print('calibrate之前')\n",
    "        print(probs)\n",
    "        print(label_name_list[np.argmax(probs)])\n",
    "        print('calibrate之后')\n",
    "        print(calibrate_label_probs)\n",
    "        print(label_name_list[np.argmax(calibrate_label_probs)])\n",
    "    \n",
    "    return calibrate_label_probs, label_name_list[np.argmax(calibrate_label_probs)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1446b688-fc9d-437a-84ba-526e5765f78c",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2023-09-01T03:15:42.975176Z",
     "iopub.status.busy": "2023-09-01T03:15:42.974848Z",
     "iopub.status.idle": "2023-09-01T03:15:42.982756Z",
     "shell.execute_reply": "2023-09-01T03:15:42.982095Z",
     "shell.execute_reply.started": "2023-09-01T03:15:42.975155Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 标注器\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_few_shot_example(clean_set, mode = 'random', num = 3):\n",
    "    few_shots = []\n",
    "    \n",
    "    if mode == 'random':\n",
    "        # clean set：【[text, label_name]]\n",
    "        clean_indexs = range(len(clean_set))\n",
    "        few_shot_indexs = np.random.choice(clean_indexs, size = num, replace=False)\n",
    "        for index in few_shot_indexs:\n",
    "            few_shots.append(clean_set[index])\n",
    "        \n",
    "    elif mode == 'class-balance-fixed':\n",
    "        # 按照类别抽，每个class的样本一样，fixed说明不随机抽sample，按顺序抽\n",
    "        # clean set: {'label_name':[text]}\n",
    "        for label_name in sorted(clean_set.keys()):\n",
    "            samples = clean_set[label_name][:num]\n",
    "            for sample in samples:\n",
    "                few_shots.append([sample, label_name])\n",
    "            \n",
    "    return few_shots\n",
    "\n",
    "def annotator(x, mode, p_calibrate, zh_label_name_list, few_shot_examples, print_log):\n",
    "    # input：x，output：y（id）\n",
    "    y_pred = ''\n",
    "    if MODEL_NAME in ['qwen-7b-chat']:\n",
    "        \n",
    "        if mode == 'channel':\n",
    "            out = llm_annotator_channel(x, zh_label_name_list, few_shot_data=few_shot_examples, print_log = print_log)\n",
    "        elif mode == 'prob_model':\n",
    "            _, out = llm_annotator_probmodel(x, zh_label_name_list, few_shot_data=few_shot_examples, print_log = print_log)\n",
    "        elif 'calibrate' in mode:\n",
    "            _, out = llm_annotator_calibrate(x, p_calibrate, zh_label_name_list, few_shot_data = few_shot_examples, print_log = print_log)\n",
    "        elif mode in['greedy_decode', 'sample']:\n",
    "            out = llm_annotator_generative_cls(x, mode, zh_label_name_list, few_shot_data = few_shot_examples, self_consistency_num = self_consistency_num, print_log=print_log)\n",
    "\n",
    "    elif MODEL_NAME in ['siamese_uninlu']:\n",
    "        out = siamase_annotator_cls(x, zh_label_name_list, print_log=print_log)\n",
    "    elif MODEL_NAME in ['paddle_nlp']:\n",
    "        out = paddle_annotator_cls(x, zh_label_name_list, print_log=print_log)\n",
    "            \n",
    "    if len(out) > 0:\n",
    "        if DATASET in ['tnews', 'nlpcc2014_task2']:\n",
    "            y_pred = en2id[zh2en[out]]\n",
    "            \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47c88d3-0d9b-416c-8bfd-62c358326d85",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 调参\n",
    "from sklearn.metrics import accuracy_score\n",
    "import jieba\n",
    "\n",
    "# evaluate通用设置\n",
    "do_eval = True\n",
    "print_log = False\n",
    "runs = 5\n",
    "sample_max_length = 256 # 单input进行right-truncate，防止超显存\n",
    "\n",
    "# FSL设置\n",
    "use_fsl = True\n",
    "fsl_example_num = 4 # random时，指总sample数；class时，指每类sample数\n",
    "fsl_example_mode = 'random' # [random, class-balance-fixed]\n",
    "\n",
    "# 标注模式\n",
    "mode = 'prob_model' #[greedy_decode, sample, channel, prob_model, calibrate_cc, calibrate_dc],后4种目前只支持qwen\n",
    "p_calibrate = None\n",
    "\n",
    "# CC的参数\n",
    "content_free_tokens = ['', '空', '无']\n",
    "\n",
    "# DC的参数\n",
    "domain_words = []\n",
    "dc_len = 0\n",
    "dc_times = 20\n",
    "\n",
    "# sample的参数\n",
    "self_consistency_num = 1\n",
    "\n",
    "def evaluate(test, sample_max_length, clean_pool, mode, p_calibrate, zh_label_name_list, few_shot_examples, print_log):\n",
    "    y_truth_list, y_pred_list = [], []\n",
    "        \n",
    "    for sample_i in tqdm(range(len(test))):\n",
    "        sample = test[sample_i]\n",
    "        x, y_truth = sample[0], sample[1]\n",
    "        if print_log:\n",
    "            print('y_truth:', en2zh[id2en[y_truth]])\n",
    "        y_pred = annotator(x[:sample_max_length], mode, p_calibrate, zh_label_name_list, few_shot_examples, print_log)\n",
    "        if isinstance(y_pred, int) or (isinstance(y_pred, str) and len(y_pred) > 0):\n",
    "            y_truth_list.append(y_truth)\n",
    "            y_pred_list.append(y_pred)\n",
    "            \n",
    "            if (y_truth != y_pred) and print_log:\n",
    "                print('预测错误')\n",
    "\n",
    "    accuracy = accuracy_score(np.array(y_truth_list), np.array(y_pred_list))\n",
    "    \n",
    "    print(f'coverage:{len(y_pred_list) / len(test)}')\n",
    "    print(f'accuracy:{accuracy}')\n",
    "\n",
    "if do_eval:\n",
    "    if DATASET in ['tnews']:\n",
    "        evaluation_set = train[:200]\n",
    "        \n",
    "        dc_lens = []\n",
    "        for d in evaluation_set:\n",
    "            d_words = jieba.lcut(d[0], HMM=False)\n",
    "            dc_lens.append(len(d_words))\n",
    "            domain_words += d_words\n",
    "                    \n",
    "        dc_len = int(sum(dc_lens) / len(dc_lens))\n",
    "\n",
    "    elif DATASET in ['nlpcc2014_task2']:\n",
    "        evaluation_set = train[:300]\n",
    "        \n",
    "        dc_lens = []\n",
    "        for d in evaluation_set:\n",
    "            d_words = jieba.lcut(d[0], HMM=False)\n",
    "            dc_lens.append(len(d_words))\n",
    "            domain_words += d_words\n",
    "                    \n",
    "        dc_len = int(sum(dc_lens) / len(dc_lens))\n",
    "        \n",
    "    for i in range(runs):\n",
    "                \n",
    "        if fsl_example_mode == 'random':\n",
    "            clean_pool = []\n",
    "            for d in val[:100]:\n",
    "                clean_pool.append([d[0], en2zh[id2en[d[1]]]])\n",
    "        elif 'class-balance' in fsl_example_mode:\n",
    "            clean_pool = {}\n",
    "            for d in val:\n",
    "                text, label_name = d[0], en2zh[id2en[d[1]]]\n",
    "                if label_name not in clean_pool:\n",
    "                    clean_pool[label_name] = [text]\n",
    "                else:\n",
    "                    clean_pool[label_name].append(text)    \n",
    "                \n",
    "        if use_fsl:\n",
    "            few_shot_examples = get_few_shot_example(clean_pool, mode = fsl_example_mode, num = fsl_example_num)\n",
    "        else:\n",
    "            few_shot_examples = []\n",
    "\n",
    "        if mode == 'calibrate_cc':\n",
    "            p_calibrate = get_p_calibrate_CC(zh_label_name_list, few_shot_data = few_shot_examples, content_free_tokens = content_free_tokens, print_log = print_log)\n",
    "            if print_log:\n",
    "                print('p_calibrate')\n",
    "                for p, n in zip(p_calibrate, zh_label_name_list):\n",
    "                    print(p, n)    \n",
    "        elif mode == 'calibrate_dc':\n",
    "            p_calibrate = get_p_calibrate_DC(zh_label_name_list, dc_len, dc_times, domain_words, few_shot_data = few_shot_examples, print_log = print_log)\n",
    "            if print_log:\n",
    "                print('p_calibrate')\n",
    "                for p, n in zip(p_calibrate, zh_label_name_list):\n",
    "                    print(p, n)    \n",
    "                    \n",
    "        evaluate(evaluation_set, sample_max_length, clean_pool, mode, p_calibrate, zh_label_name_list, few_shot_examples, print_log)\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb50a5a-d025-499f-b505-07d72c78a68c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a74b4-6cb5-488f-a45b-18740905d84a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb9e89-8015-4be7-baec-8bcd8c7e41c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fda781d-6420-4d4a-b1ab-5b827dd2d273",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
